{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 손실 함수\n",
    "#### 4.2.1 평균 제곱 오차\n",
    "$$E = \\frac{1}{2} \\sum_{k}^{} (y_k - t_k)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 신경망 출력 \n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 레이블 (원-핫 인코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2 ?\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 7 ?\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.2.2 교차 엔트로피\n",
    "$$E = - \\sum_{k}^{} t_k\\log y_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tk가 1일 때의 yk의 자연로그를 계산해서 정답을 추정 (오류가 0인 경우, log1 = 0) \n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2 ?\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 7 ?\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.2.3 미니배치 학습\n",
    "$$E = -\\frac{1}{N} \\sum_{n} \\sum_{k} t_k\\log y_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 데이터셋을 읽어옴\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 무작위로 10개 추출\n",
    "print(np.random.choice(60000, 10))\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(x_batch)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 0과 1의 원-핫 인코딩일 경우의 교차 엔트로피\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1: # y가 1차원, 데이터 하나당 교차 엔트로피의 오차를 구하는 경우는 reshape\n",
    "        t = t.reshape(1, t.size) # 정답 레이블 \n",
    "        y = t.reshape(1, y.size) # 신경망 출력\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size # 배치의 크기만큼 정규화해서 교차 엔트로피를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 숫자 레이블일 경우의 교차 엔트로피\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size # 레이블일 때는 레이블에 해당하는 바로 읽고 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 수치 미분\n",
    "#### 4.3.1 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.float64(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.3.2 수치 미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tx1, tx2 = 5, 10\n",
    "tf1 = tangent_line(function_1, tx1)\n",
    "tf2 = tangent_line(function_1, tx2)\n",
    "ty1 = tf1(x)\n",
    "ty2 = tf2(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, ty1)\n",
    "plt.plot(x, ty2)\n",
    "plt.plot(x, (lambda t: numerical_diff(function_1, tx1)*t)(x))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.3.3 편미분\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2 $$\n",
    "$$\\frac{\\partial f}{\\partial x_0} or \\frac{\\partial f}{\\partial x_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 # or return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x0 = 3, x1 = 4일 때, x0에 대한 편미분 \n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "# x0 = 3, x1 = 4일 때, x1에 대한 편미분 \n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "print(numerical_diff(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 기울기\n",
    "$$\\left(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        fxh1 = f(x)\n",
    "        v = x[idx]\n",
    "        x[idx] = v + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = v - h\n",
    "        fxh2 = f(x)\n",
    "    \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = v\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad1 = numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "grad2 = numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "grad3 = numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(grad1)\n",
    "print(grad2)\n",
    "print(grad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x0 = np.arange(-2, 2.5, 0.25)\n",
    "x1 = np.arange(-2, 2.5, 0.25)\n",
    "# print(\"x0:\", x0)\n",
    "# print(\"x1:\", x1)\n",
    "X, Y = np.meshgrid(x0, x1)\n",
    "# print(\"X:\", X)\n",
    "X = X.flatten()\n",
    "Y = Y.flatten()\n",
    "# print(\"X.flatten:\", X)\n",
    "grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "\n",
    "plt.figure()\n",
    "plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "plt.xlim([-2, 2])\n",
    "plt.ylim([-2, 2])\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4.1 경사법(경사 하강법)\n",
    "$$ x_0 = x_0 - \\eta{\\frac{\\partial f}{\\partial x_0}}$$\n",
    "$$ x_1 = x_1 - \\eta{\\frac{\\partial f}{\\partial x_1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# f 최적화 함수\n",
    "# lr learning rate\n",
    "# step_num 반복수 \n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 학습률이 너무 큰 예 : lr=10.0\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 학습률이 너무 작은 예: lr=1e-10\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4.2 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{W} =  \\begin{pmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\frac{\\partial L}{\\partial W}} =  \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{31}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial W_{22}} & \\frac{\\partial L}{\\partial W_{32}}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dW = numerical_gradient(lambda W: net.loss(x,t), net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.5 학습 알고리즘 구현하기\n",
    "1단계. 미니배치 -> 2단계. 기울기 산출 -> 3단계. 매개변수 갱신 -> 4단계. 1~3단계 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "하나의 입력 $x$에 대한 출력을 $d$라고 하고, 다음과 같이 입출력의 쌍이 여러 개 주어졌을 때, \n",
    "\n",
    "$$\\{(x_1, d_1), (x_2, d_2), ..., (x_n, d_n)\\}$$\n",
    "\n",
    "이 입출력 쌍 $(x, d)$ 하나하나를 **훈련 샘플(training sample)** 이라 부르고, 그 집합을 **훈련 데이터(training data)**라고 한다.\n",
    "\n",
    "이 때, 신경망의 가중치 $w$를 조정하여, 모든 입출력 쌍 $(x_n, d_n)(n = 1, ..., N)$ 입력 $x_n$이 주어진 신경망의 출력 $y(x_n; w)$이 최대한 $d_n$과 가까워지도록 하는 과정을 **학습**이라고 부른다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "두 개의 레이어이므로 그 구성은 다음 그림과 같다. \n",
    "\n",
    "<img src=http://neuralnetworksanddeeplearning.com/images/tikz35.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2층 구조의 신경망이다. l=1인 층을 입력층, l=2인 층을 은닉층, l=3인 층을 출력층이라고 부른다.\n",
    "\n",
    "각 층 $l + 1$ 의 유닛의 출력 $a^{l+1}$은 다음과 같이 일반화할 수 있다.\n",
    "$$u^{l+1} = W^(l+1)z^(l) + b^(l+1)$$\n",
    "$$a^{l+1} = f(u^(l+1))$$\n",
    "\n",
    "#### 활성화 함수\n",
    "활성화 함수는 보통 실수 전체를 정의역으로 시그모이드 함수, 램프 함수, 맥스 아웃 함수 등이 있다.\n",
    "- 시그모이드 함수: 실수 전체를 정의역으로, (0, 1)을 치역으로 가진다.\n",
    "$$ f(u) = \\frac{1}{1 + e^{-u}} $$\n",
    "\n",
    "- 쌍곡선 정접 함수: (-1, 1)의 치역을 갖는다.\n",
    "$$ f(u) = tanh(u) $$\n",
    "\n",
    "- 램프 함수 (ramp function, rectified linear function): u < 0인 부분을 0으로 바꾼 단순 함수이다. 단순하고 계산량이 적다. 학습이 빠르고 최종결과도 더 좋은 경우가 많아 가장 많이 사용되고 있다.\n",
    "$$ f(u) = max(u, 0) $$\n",
    "\n",
    "- 맥스아웃 함수: 각각의 총 입력을 유닛별로 따로 계산한 후, 그 중한다. 최대값을 유닛의 출력으로 한다.   \n",
    "$$ f(u_j) = max (u_jk) (k=1,...,K) $$\n",
    "\n",
    "- 항등 사상: 회귀 문제를 위한 신경망에서 사용한다.\n",
    "$$ f(u) = u $$\n",
    "\n",
    "- 소프트맥스 함수: 클래스 분류를 위한 신경망에서 사용한다. 출력의 합이 항상 1이 된다. 모든 유닛의 총 입력으로부터 결정되는 점이 다른 활성화 함수와 다르다. 지수 함수에 따른 오버플로우를 방지하기 위해, 보통 입력값 중 최대값을 기준으로 정규화한다.\n",
    "$$ f(u) = \\frac{\\exp({u_k})}{\\sum_{j=1}^{k} \\exp({u_j})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 오차 함수\n",
    "오차함수의 종류는 다음과 같다\n",
    "\n",
    "| 문제의 유형 | 활성화 함수 | 오차 함수\n",
    "| :- |:-----------------:| ----------------\n",
    "| 회귀 | 항등사상 |  $ 제곱 오차 $ \n",
    "| 이진 분류 |  시그모이드 (또는 로지스틱함수) |$ 교차 엔트로피 $ \n",
    "| 클래스 분류 | 소프트맥스 함수 | 교차 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 신경망 파라미터 벡터\n",
    "신경망 유닛의 바이어스와 가중치의 모든 성분으로 구성되는 벡터를$\\textbf{w}$를 정의해 $y(\\textbf{x};\\textbf{w})$로 표기한다. (곳에 따라 바이어스와 가중치를 구분하지 않고 일괄 $\\theta$로 표기하기도 한다.)\n",
    "\n",
    "#### 회귀 \n",
    "제곱오차를 합한 후 이를 반으로 나눈 값을 계산하고, 이 값이 최소가 되는 $w$를 선택한다. 2로 나누는 것은 미분하면 2가 곱해지는 것을 상쇄하기 위해서이다. \n",
    "\n",
    "$$ E(w) = \\frac{1}{2}\\sum_{i=1}^{n} \\parallel d_n - y(\\textbf{x_n};w) \\parallel^2 $$\n",
    "\n",
    "#### 이진 분류\n",
    "- 통계에서 확률변수 $x_1, x_2, \\dots, x_n$가 서로 독립이고, 확률밀도함수 $f(x; \\theta)$의 확률표본이면 우도함수는 다음과 같다.\n",
    "\n",
    "$$ L(\\theta \\mid x_1, ..., x_n) = f(x_1, x_2, ..., x_n \\mid \\theta) = \\prod_{i=1}^{n} f(x_i \\mid \\theta ) $$\n",
    "\n",
    "- 위와 같이 우도함수는 곱해진 형태이기 때문에 로그를 취하여 더해진 형태로 표시한다. 보통 비선형 데이터나 정규 분포가 아닌 경우, 로그를 취해 선형 데이터로 변경한다. 이렇게 하면 원래의 데이터에 로그변환을 취한 것의 차분이 원데이터의 변화율이 된다.  \n",
    "\n",
    "$$ l(\\theta) = logL(\\theta) = \\sum_{i=1}^{n}logf(x_i \\mid \\theta) $$\n",
    "\n",
    "- 주어진 입력 x에 따라 두 종류를 구별할 때, $d = 0$은 여자, $d = 1$을 남자라고 하면, 종류는 이진 변수 $ d \\in \\{ 0, 1\\} $가 된다. \n",
    "- 주어진 $\\textbf{x}$에 대한 $d$의 추정은 사후확률 $p(d = 1 \\mid \\textbf{x})$로 모델화하여, 그 값이 0.5를 넘으면 d = 1, 그렇지 않으면 d = 0 으로 판단한다.\n",
    "- 신경망 전체의 입출력함수 $y(\\textbf{x}; \\textbf{w})$를 $p(d = 1 \\mid \\textbf{x})$로 수식화하여 데이터의 사후분포가 가장 잘맞는 신경망의 파라미터 $w$를 조정한다. 즉, $\\textbf{w}$에 대한 우도를 구해, 가장 큰 $\\textbf{w}$의 값을 선택한다. 이를 최대우도법 (maximum likelihood estimation)이라 한다.   \n",
    "- 사후 분포는 다음과 같이 풀 수 있다.\n",
    "\n",
    "$$ p(d \\mid \\textbf{x} ) = p(d=1 \\mid \\textbf{x})^{d}p(d=0 \\mid x)^{1-d} $$\n",
    "\n",
    "- 확률의 총합은 1이므로, $ p(d = 0 \\mid \\textbf{x}) = 1 - y(x; w) $를 유도할 수 있다.\n",
    "- 따라서, $w$를 가장 크게 하는 최대우도 함수는 다음과 같이 주어진다.\n",
    "\n",
    "$$ L(w) \\equiv \\prod_{i=1}^{N} p(d_n \\mid \\textbf{x};\\textbf{w}) = \\\n",
    "   \\prod_{i=1}^{N} {\\{ y(\\textbf{x}_n;\\textbf{w}) \\}}^{d_n} {\\{ 1 - y(\\textbf{x}_n;\\textbf{w}) \\}}^{1-d_n} $$\n",
    "    \n",
    "- 따라서 최대화 대신 최소화하기 위해 부호를 바꾼 오차함수는 다음과 같다.\n",
    "\n",
    "$$ E(w) = -\\sum_{n=1}^{N}[d_nlog\\{y(\\textbf{x}_n;\\textbf{w})\\} + (1-d_n)log\\{(1-y(\\textbf{x}_n;\\textbf{w})\\}) ] $$\n",
    "\n",
    "#### 클래스 분류\n",
    "클래스 분류도 그 원리는 이진 분류와 동일히다. \n",
    "\n",
    "- 클래스를 $C_1, \\dots, C_k$라고 했을 때, 출력 $y_k$은 입력 $\\textbf{x}$가 클래스 $C_k$에 속할 확률을 나타낸다. 이 때, $y_1, \\dots, y_k$의 총합은 항상 1이 된다. 출력 함수 $y_k$로 사용되는 소프트맥수 함수는 다음과 같다.\n",
    "\n",
    "$$ y_k = \\frac{exp(u_k)}{\\sum_{j=1}^{K}exp(u_j)} $$\n",
    "\n",
    "- 신경망의 이진값을 $K$개 열거한 벡터 $\\textbf{d}_n = [d_n1, ..., d_nk]^\\top $로 표현할 때\n",
    "- 입력 $\\textbf{x}_n$이 숫자 2이고, 정답 클래스가 $C_3$일 때, 이 입력의 목표 출력은 다음과 같다.\n",
    "\n",
    "$$ \\textbf{d}_n = [ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 ]^\\top $$\n",
    "\n",
    "- 이에 대한 사후 분포는 다음과 같다. \n",
    "$$ p(\\textbf{d}|x) = \\prod_{k=1}^{K}p(C_k|x)^{d_k}) $$\n",
    "\n",
    "- 따라서 우도함수에 소프트맥스 함수를 이용해서 표기하면 다음과 같다.\n",
    "$$ L(w) = \\prod_{n=1}^{N}p(d_n \\mid \\textbf{x_n};\\textbf{w}) = \\prod_{n=1}^{N}\\prod_{k=1}^{K}p(\\textbf{C}_k \\mid\\textbf{x}_n)^{d_nk} = \\prod_{n=1}^{N}\\prod_{k=1}^{K}(y_k(\\textbf{x};\\textbf{w}))^{d_nk} $$ \n",
    "\n",
    "- 이 우도에 로그를 취하여 부호를 반전한 것을 오차함수로 삼는다. 이를 교차 엔트로피라고 한다. \n",
    "\n",
    "$$ E(w) = - \\sum_{n=1}^{N}\\sum_{k=1}^{K}d_{nk}logy_{k}(x_n;w) $$\n",
    "\n",
    "- 데이터가 $\\textbf{N}$개이고, 클래스가 $\\textbf{K}$개 일 때, $(\\textbf{x}_n;\\textbf{w})$은 신경망 출력이이다. $d_nk$은 n번째 데이터의 k클래스에 대한 값으로 $k$에 해당하는 원소만 1이고, 나머지는 0이다. 이를 원-핫 인코딩이라고 한다. 결과적으로, $t_k$가 1일 때의 신경망의 출력 $y_k$에 대한 자연로그를 계산하는데, 정답에 가까울수록 오차는 작아지고, 정답에 멀어질수록 오차는 커진다. 마지막을 $\\textbf{N}$으로 나누어 정규화하여 훈련 데이터의 개수에 관련없는 평균 손실함수를 구하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from common.gradient import numerical_gradient\n",
    "from common.functions import *\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        # 100개의 훈련 데이터(28x28)가 들어있는 행벡터 형태를 이루는 x(100, 784)와 W1(784, 100)벡터의 내적을 구한 후, \n",
    "        # b1만큼 트랜스포밍한 벡터 a1(100, 100)를 구한다. \n",
    "        \n",
    "        # 100x100에 큰 의미는 없다. 단지 은닉층의 개수만큼의 크기를 지닐 뿐이다. \n",
    "        # 은닉층이 50개의 행벡터이면 a1의 차원은 50x50이 된다.\n",
    "        # 은닉층을 딥하게 더 추가할 수 있다.\n",
    "        a1 = np.dot(x, W1) + b1 \n",
    "        \n",
    "        # a1의 출력값은 음수부터 양수까지 값이 다양한 범위를 지닌다. 따라서 다음층에 전달하기 위해 활성화 함수로 값을 변환시킨다.  \n",
    "        # 시그모이드 함수는 0부터 1사이의 양수의 값으로 변환시킨다. \n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        # z1(100, 100) 벡터와 W2 (100, 10) 벡터의 내적을 구해 벡터 a2(100, 10)를 얻는다. \n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        \n",
    "        # 최종 레이블 분류를 위해 소프트맥스 함수를 사용한다. \n",
    "        \"\"\"\n",
    "        실제 사용된 함수는 같다. \n",
    "        def softmax(x)\n",
    "            if x.ndim == 2: # 가중치 데이터가 2차원이면  \n",
    "                x = x.T # 오버플로우를 방지하기 위해, 100개의 행벡터를 10개의 행벡터로 변환한다. (100, 10) => (10, 100)\n",
    "                x = x - np.max(x, axis=0) # 오버플로우 방지, 100개의 훈련된 데이터의 각 열에서 10개의 분류값에 대한 최대값을 빼준다.\n",
    "                                          # axis에 대한 numpy 개념을 알고 있어야 한다.\n",
    "                y = np.exp(x) / np.sum(np.exp(x), axis=0) # 소프트맥스 공식을 적용한다.\n",
    "                return y.T # (10, 100) => (100, 10) 원래대로 돌려 손실함수, 비용함수 계산시 레이블 데이터의 차원과 맞춘다. \n",
    "            x = x - np.max(x)\n",
    "            return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "        \"\"\"\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "        \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # 정답 레이블이 1일 때의 해당하는 추정치를 교차 엔트로피를 이용해 계산한다.\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    # 미분에 의한 방법: 시간이 오래 걸림\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \n",
    "        # 입력 데이터와 정답 레이블의 손실 함수를 구함\n",
    "        # 매개변수로 전달받은 \n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        # W1(784, 100), W2(100, 10), b1(100,), b2(10,) 등 \n",
    "        # 다차원 배열로 이루어진 가중치 변수에 대해 기울기를 구한다.\n",
    "        \n",
    "        \"\"\"\n",
    "        실제로 사용된 함수는 다음과 같다.\n",
    "        \n",
    "        def numerical_gradient(f, x):\n",
    "            h = 1e-4 # 0.0001\n",
    "            grad = np.zeros_like(x)\n",
    "            \n",
    "            # 다차원 인덱싱 반복자를 반환한다. \n",
    "            # (100, 10)이면 (0,0)에서 (99, 9)까지의 1000개의 반복자를 생성할 수 있다.\n",
    "\n",
    "            it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                idx = it.multi_index  \n",
    "                tmp_val = x[idx] # 다차원 배열의 인덱싱에 해당하는 값을 읽어와 \n",
    "                \n",
    "                # 오차가 없는 수식에 기반한 진정한 미분인 해석적 미분을 구한다.  \n",
    "                # 이 때, 그것도 모잘라 오차를 줄이기 위해 전방 차분이 아닌, 중앙 차분을 읽어오고 있다.\n",
    "                x[idx] = float(tmp_val) + h \n",
    "                fxh1 = f(x) # f(x+h)\n",
    "\n",
    "                x[idx] = tmp_val - h\n",
    "                fxh2 = f(x) # f(x-h)\n",
    "                grad[idx] = (fxh1 - fxh2) / (2*h) # 오차가 없는 진정한 미분, 해석적 미분값을 구한다. \n",
    "\n",
    "                x[idx] = tmp_val\n",
    "                it.iternext\n",
    "            return grad\n",
    "        \"\"\"\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 오차 역전파에 의한 방법\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 1000번 돌린다. \n",
    "for i in range(iters_num):\n",
    "    # 600000개의 훈련데이터에서 100개를 무작위로 선택한다. \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask] # 무작위로 선택된 100개의 입력 데이터\n",
    "    t_batch = t_train[batch_mask] # 그 결과값인 100개의 레이블 데이터\n",
    "    \n",
    "    # 미분에 의한 기울기 벡터를 구함: 속도가 매우 느리다.\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 오차 역전파에 의해 기울기 벡터를 구함\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        # 각각의 가중치를 초기 랜덤값에서 음의 기울기 방향으로 가중치를 갱신한다. (즉, 하강한다.)\n",
    "        # 이 때 학습률(learning rate)의 값으로 갱신량을 결정한다. 오버 피팅이 발생하지 않도록 적절한 값을 산정한다.\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.098, test_acc: 0.097\n",
      "train_acc: 0.775, test_acc: 0.782\n",
      "train_acc: 0.878, test_acc: 0.882\n",
      "train_acc: 0.898, test_acc: 0.900\n",
      "train_acc: 0.908, test_acc: 0.912\n",
      "train_acc: 0.914, test_acc: 0.917\n",
      "train_acc: 0.920, test_acc: 0.923\n",
      "train_acc: 0.924, test_acc: 0.926\n",
      "train_acc: 0.928, test_acc: 0.929\n",
      "train_acc: 0.931, test_acc: 0.933\n",
      "train_acc: 0.934, test_acc: 0.935\n",
      "train_acc: 0.937, test_acc: 0.938\n",
      "train_acc: 0.938, test_acc: 0.939\n",
      "train_acc: 0.941, test_acc: 0.941\n",
      "train_acc: 0.943, test_acc: 0.944\n",
      "train_acc: 0.946, test_acc: 0.945\n",
      "train_acc: 0.947, test_acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 60000개의 훈련 데이터에서 임의로 100개를 선택한다.\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc: {:0.3f}, test_acc: {:0.3f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
